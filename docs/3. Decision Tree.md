# 三、Decision Tree

## 3.1 概述

决策树算法可以从已标记的数据中自动学习出`if else`规则集，它能保证所有的规则互斥且完备，即用户的任意一种情况一定能匹配上一条规则，且该规则唯一。

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/09_%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0.jpg)

如图所示：

- 左边是收集的一系列判断是否打球的案例，包括4个特征outlook,temperature,Humidity,Wind,以及y标签是否打球，
- 通过决策树学习后得到右边的决策树

决策树的结构：由**节点和有向边**组成，而节点又分为两种：**叶子节点**和**非叶子节点**，非叶子节点主要用于对某一特征做判断，而它下面所链接的有向边表示该特征所满足的某条件，**最终的叶子节点即表示实例的预测值**(分类/回归)

**决策树学习主要分为两个阶段**，

- 决策树生成，决策树生成阶段最重要便是特征选择
- 决策树剪枝

决策树还可以看作是给定特征条件下类的条件概率分布：

- 训练时，决策树会将特征空间划分为大大小小互不相交的区域，而每个区域对应了一个类的概率分布；
- 预测时，落到某区域的样本点的类标签即是该区域对应概率最大的那个类

## 3.2 特征选择

特征选择用于选择对分类有用的特征：

- ID3选择准则是信息增益
- C4.5选择的准则是信息增益比，下面对其作介绍并实现

### 3.2.1 信息增益

> **信息增益**

首先介绍**两个随机变量之间的互信息公式**：

$MI(Y,X)=H(Y)−H(Y|X)$

- $H(X)$表示$X$的熵，$H(X)=−\sum_{i=1}^{n}p_i logp_i$,这里$p_i=P(X=x_i)$
- 条件熵$H(Y|X)$表示在已知随机变量X的条件下，随机变量$Y$的不确定性：

$H(Y|X)=∑_{i=1}^n p_iH(Y|X=x_i)$,这里$pi=P(X=x_i)$

**信息增益**就是**Y取分类标签**，**X取某一特征时的互信息**，它表示如果选择特征X对数据进行分割，可以使得分割后Y分布的熵降低多少，若降低的越多，**说明分割每个子集的Y的分布越集中，则X对分类标签Y越有用**

> **信息增益的python实现**：

```python
"""
定义计算熵的函数,封装到ml_models.utils
"""
import numpy as np
from collections import Counter
import math

def entropy(x,sample_weight=None):
    x=np.asarray(x)
    #x中元素个数
    x_num=len(x)
    #如果sample_weight为None设均设置一样
    if sample_weight is None:
        sample_weight=np.asarray([1.0]*x_num)
    x_counter={}
    weight_counter={}
    # 统计各x取值出现的次数以及其对应的sample_weight列表
    for index in range(0,x_num):
        x_value=x[index]
        if x_counter.get(x_value) is None:
            x_counter[x_value]=0
            weight_counter[x_value]=[]
        x_counter[x_value]+=1
        weight_counter[x_value].append(sample_weight[index])
    
    #计算熵
    ent=.0
    for key,value in x_counter.items():
        p_i=1.0*value*np.mean(weight_counter.get(key))/x_num
        ent+=-p_i*math.log(p_i)
    return ent

#测试
entropy([1,2])
0.6931471805599453

def cond_entropy(x, y,sample_weight=None):
    """
    计算条件熵:H(y|x)
    """
    x=np.asarray(x)
    y=np.asarray(y)
    # x中元素个数
    x_num = len(x)
    #如果sample_weight为None设均设置一样
    if sample_weight is None:
        sample_weight=np.asarray([1.0]*x_num)
    # 计算
    ent = .0
    for x_value in set(x):
        x_index=np.where(x==x_value)
        new_x=x[x_index]
        new_y=y[x_index]
        new_sample_weight=sample_weight[x_index]
        p_i=1.0*len(new_x)/x_num
        ent += p_i * entropy(new_y,new_sample_weight)
    return ent

#测试
cond_entropy([1,2],[1,2])
0.0


def muti_info(x, y,sample_weight=None):
    """
    互信息/信息增益:H(y)-H(y|x)
    """
    x_num=len(x)
    if sample_weight is None:
        sample_weight=np.asarray([1.0]*x_num)
    return entropy(y,sample_weight) - cond_entropy(x, y,sample_weight)
```

> **做一个测试，看特征的取值的个数对信息增益的影响**

```python

import random
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
#作epochs次测试
epochs=100
#x的取值的个数：2->class_num_x
class_num_x=100
#y标签类别数
class_num_y=2
#样本数量
num_samples=500
info_gains=[]
for _ in range(0,epochs):
    info_gain=[]
    for class_x in range(2,class_num_x):
        x=[]
        y=[]
        for _ in range(0,num_samples):
            x.append(random.randint(1,class_x))
            y.append(random.randint(1,class_num_y))
        info_gain.append(muti_info(x,y))
    info_gains.append(info_gain)
plt.plot(np.asarray(info_gains).mean(axis=0))
[<matplotlib.lines.Line2D at 0x21ed2625ba8>]
```

可以发现一个很有意思的现象，如果 **特征的取值的个数越多，越容易被选中，** 这比较好理解，假设一个极端情况，若对每一个实例特征$x$的取值都不同，则其$H(Y|X)$项为0，则$MI(X,Y)=H(Y)−H(Y|X)$将会取得最大值（$H(Y)$与$X$无关），**这便是ID3算法的一个痛点，为了矫正这一问题，C4.5算法利用信息增益比作特征选择**

### 3.2.2 信息增益比

信息增益比其实就是对**信息增益除以了一个x的熵**：

$MI(X,Y)/H(X)$

> **信息增益比的python实现**

```python
def info_gain_rate(x, y,sample_weight=None):
    """
    信息增益比
    """
    x_num=len(x)
    if sample_weight is None:
        sample_weight=np.asarray([1.0]*x_num)
    return 1.0 * muti_info(x, y,sample_weight) / (1e-12 + entropy(x,sample_weight))
    
```

> **接下来再作一次相同的测试**：

```python
#作epochs次测试
epochs=100
#x的取值的个数：2->class_num_x
class_num_x=100
#y标签类别数
class_num_y=2
#样本数量
num_samples=500
info_gain_rates=[]
for _ in range(0,epochs):
    info_gain_rate_=[]
    for class_x in range(2,class_num_x):
        x=[]
        y=[]
        for _ in range(0,num_samples):
            x.append(random.randint(1,class_x))
            y.append(random.randint(1,class_num_y))
        info_gain_rate_.append(info_gain_rate(x,y))
    info_gain_rates.append(info_gain_rate_)
plt.plot(np.asarray(info_gain_rates).mean(axis=0))

# 虽然整体还是上升的趋势，当相比于信息增益已经缓解了很多，将它们画一起直观感受一下：

plt.plot(np.asarray(info_gains).mean(axis=0),'r')
plt.plot(np.asarray(info_gain_rates).mean(axis=0),'y')

```

## 3.3 决策树生成

决策树的生成就是**一个递归地调用特征选择的过程**

- 首先从根节点开始，利用信息增益/信息增益比选择最佳的特征作为节点特征，由该特征的不同取值建立子节点
- 然后再对子节点调用以上方法，直到所有特征的信息增益/信息增益比均很小或者没有特征可以选择时停止，
- 最后得到一颗决策树。接下来直接进行代码实现：

``` python
import os
os.chdir('../')
from ml_models import utils
from ml_models.wrapper_models import DataBinWrapper
"""
ID3和C4.5决策树分类器的实现，放到ml_models.tree模块
"""
class DecisionTreeClassifier(object):
    class Node(object):
        """
        树节点，用于存储节点信息以及关联子节点
        """

        def __init__(self, feature_index: int = None, target_distribute: dict = None, weight_distribute: dict = None,
                     children_nodes: dict = None, num_sample: int = None):
            """
            :param feature_index: 特征id
            :param target_distribute: 目标分布
            :param weight_distribute:权重分布
            :param children_nodes: 孩子节点
            :param num_sample:样本量
            """
            self.feature_index = feature_index
            self.target_distribute = target_distribute
            self.weight_distribute = weight_distribute
            self.children_nodes = children_nodes
            self.num_sample = num_sample

    def __init__(self, criterion='c4.5', max_depth=None, min_samples_split=2, min_samples_leaf=1,
                 min_impurity_decrease=0, max_bins=10):
        """
        :param criterion:划分标准，包括id3,c4.5，默认为c4.5
        :param max_depth:树的最大深度
        :param min_samples_split:当对一个内部结点划分时，要求该结点上的最小样本数，默认为2
        :param min_samples_leaf:设置叶子结点上的最小样本数，默认为1
        :param min_impurity_decrease:打算划分一个内部结点时，只有当划分后不纯度(可以用criterion参数指定的度量来描述)减少值不小于该参数指定的值，才会对该结点进行划分，默认值为0
        """
        self.criterion = criterion
        if criterion == 'c4.5':
            self.criterion_func = utils.info_gain_rate
        else:
            self.criterion_func = utils.muti_info
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_impurity_decrease = min_impurity_decrease

        self.root_node: self.Node = None
        self.sample_weight = None
        self.dbw = DataBinWrapper(max_bins=max_bins)

    def _build_tree(self, current_depth, current_node: Node, x, y, sample_weight):
        """
        递归进行特征选择，构建树
        :param x:
        :param y:
        :param sample_weight:
        :return:
        """
        rows, cols = x.shape
        # 计算y分布以及其权重分布
        target_distribute = {}
        weight_distribute = {}
        for index, tmp_value in enumerate(y):
            if tmp_value not in target_distribute:
                target_distribute[tmp_value] = 0.0
                weight_distribute[tmp_value] = []
            target_distribute[tmp_value] += 1.0
            weight_distribute[tmp_value].append(sample_weight[index])
        for key, value in target_distribute.items():
            target_distribute[key] = value / rows
            weight_distribute[key] = np.mean(weight_distribute[key])
        current_node.target_distribute = target_distribute
        current_node.weight_distribute = weight_distribute
        current_node.num_sample = rows
        # 判断停止切分的条件

        if len(target_distribute) <= 1:
            return

        if rows < self.min_samples_split:
            return

        if self.max_depth is not None and current_depth > self.max_depth:
            return

        # 寻找最佳的特征
        best_index = None
        best_criterion_value = 0
        for index in range(0, cols):
            criterion_value = self.criterion_func(x[:, index], y)
            if criterion_value > best_criterion_value:
                best_criterion_value = criterion_value
                best_index = index

        # 如果criterion_value减少不够则停止
        if best_index is None:
            return
        if best_criterion_value <= self.min_impurity_decrease:
            return
        # 切分
        current_node.feature_index = best_index
        children_nodes = {}
        current_node.children_nodes = children_nodes
        selected_x = x[:, best_index]
        for item in set(selected_x):
            selected_index = np.where(selected_x == item)
            # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
            if len(selected_index[0]) < self.min_samples_leaf:
                continue
            child_node = self.Node()
            children_nodes[item] = child_node
            self._build_tree(current_depth + 1, child_node, x[selected_index], y[selected_index],
                             sample_weight[selected_index])

    def fit(self, x, y, sample_weight=None):
        # check sample_weight
        n_sample = x.shape[0]
        if sample_weight is None:
            self.sample_weight = np.asarray([1.0] * n_sample)
        else:
            self.sample_weight = sample_weight
        # check sample_weight
        if len(self.sample_weight) != n_sample:
            raise Exception('sample_weight size error:', len(self.sample_weight))

        # 构建空的根节点
        self.root_node = self.Node()

        # 对x分箱
        self.dbw.fit(x)

        # 递归构建树
        self._build_tree(1, self.root_node, self.dbw.transform(x), y, self.sample_weight)

    # 检索叶子节点的结果
    def _search_node(self, current_node: Node, x, class_num):
        if current_node.feature_index is None or current_node.children_nodes is None or len(
                current_node.children_nodes) == 0 or current_node.children_nodes.get(
            x[current_node.feature_index]) is None:
            result = []
            total_value = 0.0
            for index in range(0, class_num):
                value = current_node.target_distribute.get(index, 0) * current_node.weight_distribute.get(index, 1.0)
                result.append(value)
                total_value += value
            # 归一化
            for index in range(0, class_num):
                result[index] = result[index] / total_value
            return result
        else:
            return self._search_node(current_node.children_nodes.get(x[current_node.feature_index]), x, class_num)

    def predict_proba(self, x):
        # 计算结果概率分布
        x = self.dbw.transform(x)
        rows = x.shape[0]
        results = []
        class_num = len(self.root_node.target_distribute)
        for row in range(0, rows):
            results.append(self._search_node(self.root_node, x[row], class_num))
        return np.asarray(results)

    def predict(self, x):
        return np.argmax(self.predict_proba(x), axis=1)
#造伪数据
from sklearn.datasets import make_classification
data, target = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=1, n_redundant=0,
                                   n_repeated=0, n_clusters_per_class=1, class_sep=.5,random_state=21)
#训练查看效果
tree = DecisionTreeClassifier(max_bins=15)
tree.fit(data, target)
utils.plot_decision_function(data, target, tree)
```

可以发现，如果**不对决策树施加一些限制，它会尝试创造很细碎的规则去使所有的训练样本正确分类**，这无疑会使得模型过拟合，**所以接下来需要对其进行减枝操作**，避免其过拟合

## 3.4 决策树剪枝

顾名思义，剪掉一些不必要的叶子节点

Q：如何确定那些叶子节点需要去掉，哪些不需要去掉呢？

这可以通过**构建损失函数来量化**，如果剪掉某一叶子结点后损失函数能减少，则进行剪枝操作，如果不能减少则不剪枝。

> 一种简单的量化损失函数

$C_α(T)=∑_{t=1}^{∣T∣}N_tH_t(T)+α∣T∣$

这里$∣T∣$表示树T的叶结点个数，$t$是树$∣T∣$的一个叶结点，该叶节点有$N_t$个样本点，其中$k$类样本点有$N_{tk}$个，$k=1,2,3,...,K$，$H_t(T)$为叶结点$t$上的经验熵，$α≥0$为超参数，其中：

$H_t(T)=−\sum_k N_{tk}/N_t*log{N_{tk}}/N_t$
该损失函数可以分为两部分，第一部分$\sum^{∣T∣}_{t=1}N_tH_t(T)$为经验损失，第二部分$∣T∣$为结构损失，$α$为调节其平衡度的系数，如果$α$越大则模型结构越简单，越不容易过拟合，接下来进行剪枝的代码实现：

```python
def _prune_node(self, current_node: Node, alpha):
        # 如果有子结点,先对子结点部分剪枝
        if current_node.children_nodes is not None and len(current_node.children_nodes) != 0:
            for child_node in current_node.children_nodes.values():
                self._prune_node(child_node, alpha)

        # 再尝试对当前结点剪枝
        if current_node.children_nodes is not None and len(current_node.children_nodes) != 0:
            # 避免跳层剪枝
            for child_node in current_node.children_nodes.values():
                # 当前剪枝的层必须是叶子结点的层
                if child_node.children_nodes is not None and len(child_node.children_nodes) > 0:
                    return
            # 计算剪枝前的损失值
            pre_prune_value = alpha * len(current_node.children_nodes)
            for child_node in current_node.children_nodes.values():
                for key, value in child_node.target_distribute.items():
                    pre_prune_value += -1 * child_node.num_sample * value * np.log(
                        value) * child_node.weight_distribute.get(key, 1.0)
            # 计算剪枝后的损失值
            after_prune_value = alpha
            for key, value in current_node.target_distribute.items():
                after_prune_value += -1 * current_node.num_sample * value * np.log(
                    value) * current_node.weight_distribute.get(key, 1.0)

            if after_prune_value <= pre_prune_value:
                # 剪枝操作
                current_node.children_nodes = None
                current_node.feature_index = None

    def prune(self, alpha=0.01):
        """
        决策树剪枝 C(T)+alpha*|T|
        :param alpha:
        :return:
        """
        # 递归剪枝
        self._prune_node(self.root_node, alpha)
```

```python
from ml_models.tree import DecisionTreeClassifier
#训练查看效果
tree = DecisionTreeClassifier(max_bins=15)
tree.fit(data, target)
tree.prune(alpha=1.5)
utils.plot_decision_function(data, target, tree)

```

通过探索α，我们可以得到一个比较令人满意的剪枝结果，这样的剪枝方式通常又被称为**后剪枝**，即从一颗完整生成后的树开始剪枝

与其对应的还有**预剪枝**，即**在训练过程中就对其进行剪枝操作**，这通常需要另外构建一份验证集做支持，这里就不实现了，

**另外比较通常的做法是**，通过一些参数来控制模型的复杂度，

- 比如max_depth控制树的最大深度，
- min_samples_leaf控制叶子结点的最小样本数
- min_impurity_decrease控制特征划分后的最小不纯度
- min_samples_split控制结点划分的最小样本数
- 通过调节这些参数，同样可以达到剪枝的效果，比如下面通过控制叶结点的最小数量达到了和上面剪枝一样的效果：

```python
tree = DecisionTreeClassifier(max_bins=15,min_samples_leaf=3)
tree.fit(data, target)
utils.plot_decision_function(data, target, tree)
```

## 3.5 CART

### 3.5.1 CART简介

**CART树即分类回归树**(classification and regression tree)，顾名思义，它即能用作**分类任务**又能用作**回归任务**，它的应用比较广泛，通常会用作集成学习的基分类器，总得来说，它与ID3/C4.5有如下不同：

- 它是一颗二叉树；
- 特征选择的方法不一样，**CART分类树利用基尼系数做特征选择**，*CART回归树利用平方误差做特征选择*；

### 3.5.2 CART分类树

> **基尼系数**

$Gini(p)=\sum_{k=1}^{K}p_k(1−p_k)=1−\sum_{k=1}^Kp^2_k$

所以，对于给定的样本集合D，其基尼指数：

$Gini(D)=1−\sum_{k=1}^K(∣Ck∣/∣D∣)^2$

这里，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数，由于CART树是二叉树，所以对于某特征A，判断其对分类标签的贡献时，只需要**判断该特征是否等于某个取值$a$的情况**，将当前数据集分割成$D1$和$D2$两部分：

$D1={(x,y)∈D∣A(x)=a},D2=D−D1$

所以在特征$A(x)=a$的条件下，集合D的基尼指数可以定义为：

$Gini(D,A,a)=\frac{∣D1∣}{∣D∣}Gini(D1)+\frac{∣D2∣}{∣D∣}Gini(D2)$

这里$D1={(x,y)∈D∣A(x)=a},D2=D−D1$

> **CART分类树代码实现**

接下来进行CART分类树的代码实现，这里与ID3/C4.5最大的不同就是 **每次对当前结点仅进行二分处理**

```python
"""
定义计算gini系数相关的函数,代码封装到ml_models.utils
"""
import numpy as np
def gini(x, sample_weight=None):
    """
    计算基尼系数 Gini(D)
    :param x:
    :param sample_weight:
    :return:
    """
    x_num = len(x)
    # 如果sample_weight为None设均设置一样
    if sample_weight is None:
        sample_weight = np.asarray([1.0] * x_num)
    x_counter = {}
    weight_counter = {}
    # 统计各x取值出现的次数以及其对应的sample_weight列表
    for index in range(0, x_num):
        x_value = x[index]
        if x_counter.get(x_value) is None:
            x_counter[x_value] = 0
            weight_counter[x_value] = []
        x_counter[x_value] += 1
        weight_counter[x_value].append(sample_weight[index])

    # 计算gini系数
    gini_value = 1.0
    for key, value in x_counter.items():
        p_i = 1.0 * value * np.mean(weight_counter.get(key)) / x_num
        gini_value -= p_i * p_i
    return gini_value

def cond_gini(x, y, sample_weight=None):
    """
    计算条件gini系数:Gini(y,x)
    """
    x = np.asarray(x)
    y = np.asarray(y)
    # x中元素个数
    x_num = len(x)
    # 如果sample_weight为None设均设置一样
    if sample_weight is None:
        sample_weight = np.asarray([1.0] * x_num)
    # 计算
    gini_value = .0
    for x_value in set(x):
        x_index = np.where(x == x_value)
        new_x = x[x_index]
        new_y = y[x_index]
        new_sample_weight = sample_weight[x_index]
        p_i = 1.0 * len(new_x) / x_num
        gini_value += p_i * gini(new_y, new_sample_weight)
    return gini_value


def gini_gain(x, y, sample_weight=None):
    """
    gini值的增益
    """
    x_num = len(x)
    if sample_weight is None:
        sample_weight = np.asarray([1.0] * x_num)
    return gini(y, sample_weight) - cond_gini(x, y, sample_weight)
```

```python
import os
os.chdir('../')
from ml_models import utils
from ml_models.wrapper_models import DataBinWrapper
"""
CART分类树的实现，代码封装到ml_models.tree模块
"""
class CARTClassifier(object):
    class Node(object):
        """
        树节点，用于存储节点信息以及关联子节点
        """

        def __init__(self, feature_index: int = None, feature_value=None, target_distribute: dict = None,
                     weight_distribute: dict = None,
                     left_child_node=None, right_child_node=None, num_sample: int = None):
            """
            :param feature_index: 特征id
            :param feature_value: 特征取值
            :param target_distribute: 目标分布
            :param weight_distribute:权重分布
            :param left_child_node: 左孩子结点
            :param right_child_node: 右孩子结点
            :param num_sample:样本量
            """
            self.feature_index = feature_index
            self.feature_value = feature_value
            self.target_distribute = target_distribute
            self.weight_distribute = weight_distribute
            self.left_child_node = left_child_node
            self.right_child_node = right_child_node
            self.num_sample = num_sample

    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1,
                 min_impurity_decrease=0, max_bins=10):
        """
        :param criterion:划分标准，默认为gini,另外entropy表示用信息增益比
        :param max_depth:树的最大深度
        :param min_samples_split:当对一个内部结点划分时，要求该结点上的最小样本数，默认为2
        :param min_samples_leaf:设置叶子结点上的最小样本数，默认为1
        :param min_impurity_decrease:打算划分一个内部结点时，只有当划分后不纯度(可以用criterion参数指定的度量来描述)减少值不小于该参数指定的值，才会对该结点进行划分，默认值为0
        """
        self.criterion = criterion
        if criterion == 'gini':
            self.criterion_func = utils.gini_gain
        else:
            self.criterion_func = utils.info_gain_rate
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_impurity_decrease = min_impurity_decrease

        self.root_node: self.Node = None
        self.dbw = DataBinWrapper(max_bins=max_bins)

    def _build_tree(self, current_depth, current_node: Node, x, y, sample_weight):
        """
        递归进行特征选择，构建树
        :param x:
        :param y:
        :param sample_weight:
        :return:
        """
        rows, cols = x.shape
        # 计算y分布以及其权重分布
        target_distribute = {}
        weight_distribute = {}
        for index, tmp_value in enumerate(y):
            if tmp_value not in target_distribute:
                target_distribute[tmp_value] = 0.0
                weight_distribute[tmp_value] = []
            target_distribute[tmp_value] += 1.0
            weight_distribute[tmp_value].append(sample_weight[index])
        for key, value in target_distribute.items():
            target_distribute[key] = value / rows
            weight_distribute[key] = np.mean(weight_distribute[key])
        current_node.target_distribute = target_distribute
        current_node.weight_distribute = weight_distribute
        current_node.num_sample = rows
        # 判断停止切分的条件

        if len(target_distribute) <= 1:
            return

        if rows < self.min_samples_split:
            return

        if self.max_depth is not None and current_depth > self.max_depth:
            return

        # 寻找最佳的特征以及取值
        best_index = None
        best_index_value = None
        best_criterion_value = 0
        for index in range(0, cols):
            for index_value in set(x[:, index]):
                criterion_value = self.criterion_func((x[:, index] == index_value).astype(int), y, sample_weight)
                if criterion_value > best_criterion_value:
                    best_criterion_value = criterion_value
                    best_index = index
                    best_index_value = index_value

        # 如果criterion_value减少不够则停止
        if best_index is None:
            return
        if best_criterion_value <= self.min_impurity_decrease:
            return
        # 切分
        current_node.feature_index = best_index
        current_node.feature_value = best_index_value
        selected_x = x[:, best_index]

        # 创建左孩子结点
        left_selected_index = np.where(selected_x == best_index_value)
        # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
        if len(left_selected_index[0]) >= self.min_samples_leaf:
            left_child_node = self.Node()
            current_node.left_child_node = left_child_node
            self._build_tree(current_depth + 1, left_child_node, x[left_selected_index], y[left_selected_index],
                             sample_weight[left_selected_index])
        # 创建右孩子结点
        right_selected_index = np.where(selected_x != best_index_value)
        # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
        if len(right_selected_index[0]) >= self.min_samples_leaf:
            right_child_node = self.Node()
            current_node.right_child_node = right_child_node
            self._build_tree(current_depth + 1, right_child_node, x[right_selected_index], y[right_selected_index],
                             sample_weight[right_selected_index])

    def fit(self, x, y, sample_weight=None):
        # check sample_weight
        n_sample = x.shape[0]
        if sample_weight is None:
            sample_weight = np.asarray([1.0] * n_sample)
        # check sample_weight
        if len(sample_weight) != n_sample:
            raise Exception('sample_weight size error:', len(sample_weight))

        # 构建空的根节点
        self.root_node = self.Node()

        # 对x分箱
        self.dbw.fit(x)

        # 递归构建树
        self._build_tree(1, self.root_node, self.dbw.transform(x), y, sample_weight)

    # 检索叶子节点的结果
    def _search_node(self, current_node: Node, x, class_num):
        if current_node.left_child_node is not None and x[current_node.feature_index] == current_node.feature_value:
            return self._search_node(current_node.left_child_node, x, class_num)
        elif current_node.right_child_node is not None and x[current_node.feature_index] != current_node.feature_value:
            return self._search_node(current_node.right_child_node, x, class_num)
        else:
            result = []
            total_value = 0.0
            for index in range(0, class_num):
                value = current_node.target_distribute.get(index, 0) * current_node.weight_distribute.get(index, 1.0)
                result.append(value)
                total_value += value
            # 归一化
            for index in range(0, class_num):
                result[index] = result[index] / total_value
            return result

    def predict_proba(self, x):
        # 计算结果概率分布
        x = self.dbw.transform(x)
        rows = x.shape[0]
        results = []
        class_num = len(self.root_node.target_distribute)
        for row in range(0, rows):
            results.append(self._search_node(self.root_node, x[row], class_num))
        return np.asarray(results)

    def predict(self, x):
        return np.argmax(self.predict_proba(x), axis=1)

    def _prune_node(self, current_node: Node, alpha):
        # 如果有子结点,先对子结点部分剪枝
        if current_node.left_child_node is not None:
            self._prune_node(current_node.left_child_node, alpha)
        if current_node.right_child_node is not None:
            self._prune_node(current_node.right_child_node, alpha)
        # 再尝试对当前结点剪枝
        if current_node.left_child_node is not None or current_node.right_child_node is not None:
            # 避免跳层剪枝
            for child_node in [current_node.left_child_node, current_node.right_child_node]:
                # 当前剪枝的层必须是叶子结点的层
                if child_node.left_child_node is not None or child_node.right_child_node is not None:
                    return
            # 计算剪枝的前的损失值
            pre_prune_value = alpha * 2
            for child_node in [current_node.left_child_node, current_node.right_child_node]:
                for key, value in child_node.target_distribute.items():
                    pre_prune_value += -1 * child_node.num_sample * value * np.log(
                        value) * child_node.weight_distribute.get(key, 1.0)
            # 计算剪枝后的损失值
            after_prune_value = alpha
            for key, value in current_node.target_distribute.items():
                after_prune_value += -1 * current_node.num_sample * value * np.log(
                    value) * current_node.weight_distribute.get(key, 1.0)

            if after_prune_value <= pre_prune_value:
                # 剪枝操作
                current_node.left_child_node = None
                current_node.right_child_node = None
                current_node.feature_index = None
                current_node.feature_value = None

    def prune(self, alpha=0.01):
        """
        决策树剪枝 C(T)+alpha*|T|
        :param alpha:
        :return:
        """
        # 递归剪枝
        self._prune_node(self.root_node, alpha)
```

```python
#造伪数据
from sklearn.datasets import make_classification
data, target = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=1, n_redundant=0,
                                   n_repeated=0, n_clusters_per_class=1, class_sep=.5,random_state=21)
#训练并查看效果
tree = CARTClassifier()
tree.fit(data, target)
utils.plot_decision_function(data, target, tree)

## 一样的，如果不加以限制，同样会存在过拟合现象，所以可以剪枝...

#剪枝
tree.prune(5)
utils.plot_decision_function(data, target, tree)
```

### 3.5.3 CART回归树

> **CART回归树的实现**

**回归树的特征选择是使用的平方误差**，即选择一个特征$j$和一个取值$s$,将训练集按$X_j≤s$和$X_j>s$分为两部分，寻找使这两部分的误差平方之和下降最多的$j,s$，这个过程可以描述如下：

$min_{j,s}[min_{c1}\sum_(x_i∈R_1(j,s))(y_i−c_1)^2+min_{c2}\sum_{x_i∈R_2(j,s)}(y_i−c2)^2]$

这里$R_1(j,s)={x∣x^j≤s},R_2(j,s)={x∣x^j>s},c_1=ave(y_i∣x_i∈R_1(j,s)),c_2=ave(y_i∣x_i∈R_2(j,s))$

> **代码实现**：

```python
"""
平方误差相关函数，封装到ml_models.utils
"""
def square_error(x, sample_weight=None):
    """
    平方误差
    :param x:
    :param sample_weight:
    :return:
    """
    x = np.asarray(x)
    x_mean = np.mean(x)
    x_num = len(x)
    if sample_weight is None:
        sample_weight = np.asarray([1.0] * x_num)
    error = 0.0
    for index in range(0, x_num):
        error += (x[index] - x_mean) * (x[index] - x_mean) * sample_weight[index]
    return error


def cond_square_error(x, y, sample_weight=None):
    """
    计算按x分组的y的误差值
    :param x:
    :param y:
    :param sample_weight:
    :return:
    """
    x = np.asarray(x)
    y = np.asarray(y)
    # x中元素个数
    x_num = len(x)
    # 如果sample_weight为None设均设置一样
    if sample_weight is None:
        sample_weight = np.asarray([1.0] * x_num)
    # 计算
    error = .0
    for x_value in set(x):
        x_index = np.where(x == x_value)
        new_y = y[x_index]
        new_sample_weight = sample_weight[x_index]
        error += square_error(new_y, new_sample_weight)
    return error


def square_error_gain(x, y, sample_weight=None):
    """
    平方误差带来的增益值
    :param x:
    :param y:
    :param sample_weight:
    :return:
    """
    x_num = len(x)
    if sample_weight is None:
        sample_weight = np.asarray([1.0] * x_num)
    return square_error(y, sample_weight) - cond_square_error(x, y, sample_weight)

```

```python
"""
CART回归树实现，封装到ml_models.tree
"""
class CARTRegressor(object):
    class Node(object):
        """
        树节点，用于存储节点信息以及关联子节点
        """

        def __init__(self, feature_index: int = None, feature_value=None, y_hat=None, square_error=None,
                     left_child_node=None, right_child_node=None, num_sample: int = None):
            """
            :param feature_index: 特征id
            :param feature_value: 特征取值
            :param y_hat: 预测值
            :param square_error: 当前结点的平方误差
            :param left_child_node: 左孩子结点
            :param right_child_node: 右孩子结点
            :param num_sample:样本量
            """
            self.feature_index = feature_index
            self.feature_value = feature_value
            self.y_hat = y_hat
            self.square_error = square_error
            self.left_child_node = left_child_node
            self.right_child_node = right_child_node
            self.num_sample = num_sample

    def __init__(self, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_std=1e-3,
                 min_impurity_decrease=0, max_bins=10):
        """
        :param criterion:划分标准，目前仅有平方误差
        :param max_depth:树的最大深度
        :param min_samples_split:当对一个内部结点划分时，要求该结点上的最小样本数，默认为2
        :param min_std:最小的标准差
        :param min_samples_leaf:设置叶子结点上的最小样本数，默认为1
        :param min_impurity_decrease:打算划分一个内部结点时，只有当划分后不纯度(可以用criterion参数指定的度量来描述)减少值不小于该参数指定的值，才会对该结点进行划分，默认值为0
        """
        self.criterion = criterion
        if criterion == 'mse':
            self.criterion_func = utils.square_error_gain
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_std = min_std
        self.min_impurity_decrease = min_impurity_decrease

        self.root_node: self.Node = None
        self.dbw = DataBinWrapper(max_bins=max_bins)

    def _build_tree(self, current_depth, current_node: Node, x, y, sample_weight):
        """
        递归进行特征选择，构建树
        :param x:
        :param y:
        :param sample_weight:
        :return:
        """
        rows, cols = x.shape
        # 计算当前y的加权平均值
        current_node.y_hat = np.dot(sample_weight / np.sum(sample_weight), y)
        current_node.num_sample = rows
        # 判断停止切分的条件
        current_node.square_error = np.dot(y - np.mean(y), y - np.mean(y))
        if np.sqrt(current_node.square_error / rows) <= self.min_std:
            return

        if rows < self.min_samples_split:
            return

        if self.max_depth is not None and current_depth > self.max_depth:
            return

        # 寻找最佳的特征以及取值
        best_index = None
        best_index_value = None
        best_criterion_value = 0
        for index in range(0, cols):
            for index_value in sorted(set(x[:, index])):
                criterion_value = self.criterion_func((x[:, index] <= index_value).astype(int), y, sample_weight)
                if criterion_value > best_criterion_value:
                    best_criterion_value = criterion_value
                    best_index = index
                    best_index_value = index_value

        # 如果criterion_value减少不够则停止
        if best_index is None:
            return
        if best_criterion_value <= self.min_impurity_decrease:
            return
        # 切分
        current_node.feature_index = best_index
        current_node.feature_value = best_index_value
        selected_x = x[:, best_index]

        # 创建左孩子结点
        left_selected_index = np.where(selected_x <= best_index_value)
        # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
        if len(left_selected_index[0]) >= self.min_samples_leaf:
            left_child_node = self.Node()
            current_node.left_child_node = left_child_node
            self._build_tree(current_depth + 1, left_child_node, x[left_selected_index], y[left_selected_index],
                             sample_weight[left_selected_index])
        # 创建右孩子结点
        right_selected_index = np.where(selected_x > best_index_value)
        # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
        if len(right_selected_index[0]) >= self.min_samples_leaf:
            right_child_node = self.Node()
            current_node.right_child_node = right_child_node
            self._build_tree(current_depth + 1, right_child_node, x[right_selected_index], y[right_selected_index],
                             sample_weight[right_selected_index])

    def fit(self, x, y, sample_weight=None):
        # check sample_weight
        n_sample = x.shape[0]
        if sample_weight is None:
            sample_weight = np.asarray([1.0] * n_sample)
        # check sample_weight
        if len(sample_weight) != n_sample:
            raise Exception('sample_weight size error:', len(sample_weight))

        # 构建空的根节点
        self.root_node = self.Node()

        # 对x分箱
        self.dbw.fit(x)

        # 递归构建树
        self._build_tree(1, self.root_node, self.dbw.transform(x), y, sample_weight)

    # 检索叶子节点的结果
    def _search_node(self, current_node: Node, x):
        if current_node.left_child_node is not None and x[current_node.feature_index] <= current_node.feature_value:
            return self._search_node(current_node.left_child_node, x)
        elif current_node.right_child_node is not None and x[current_node.feature_index] > current_node.feature_value:
            return self._search_node(current_node.right_child_node, x)
        else:
            return current_node.y_hat

    def predict(self, x):
        # 计算结果概率分布
        x = self.dbw.transform(x)
        rows = x.shape[0]
        results = []
        for row in range(0, rows):
            results.append(self._search_node(self.root_node, x[row]))
        return np.asarray(results)

    def _prune_node(self, current_node: Node, alpha):
        # 如果有子结点,先对子结点部分剪枝
        if current_node.left_child_node is not None:
            self._prune_node(current_node.left_child_node, alpha)
        if current_node.right_child_node is not None:
            self._prune_node(current_node.right_child_node, alpha)
        # 再尝试对当前结点剪枝
        if current_node.left_child_node is not None or current_node.right_child_node is not None:
            # 避免跳层剪枝
            for child_node in [current_node.left_child_node, current_node.right_child_node]:
                # 当前剪枝的层必须是叶子结点的层
                if child_node.left_child_node is not None or child_node.right_child_node is not None:
                    return
            # 计算剪枝的前的损失值
            pre_prune_value = alpha * 2 + \
                              (0.0 if current_node.left_child_node.square_error is None else current_node.left_child_node.square_error) + \
                              (0.0 if current_node.right_child_node.square_error is None else current_node.right_child_node.square_error)
            # 计算剪枝后的损失值
            after_prune_value = alpha + current_node.square_error

            if after_prune_value <= pre_prune_value:
                # 剪枝操作
                current_node.left_child_node = None
                current_node.right_child_node = None
                current_node.feature_index = None
                current_node.feature_value = None
                current_node.square_error = None

    def prune(self, alpha=0.01):
        """
        决策树剪枝 C(T)+alpha*|T|
        :param alpha:
        :return:
        """
        # 递归剪枝
        self._prune_node(self.root_node, alpha)
```

```python
#构造数据
data = np.linspace(1, 10, num=100)
target = np.sin(data) + np.random.random(size=100)#添加噪声
data = data.reshape((-1, 1))
tree = CARTRegressor(max_bins=50)
tree.fit(data, target)
import matplotlib.pyplot as plt
plt.scatter(data, target)
plt.plot(data, tree.predict(data), color='r')

#剪枝
tree.prune(1)
plt.scatter(data, target)
plt.plot(data, tree.predict(data), color='r')
```

## 参考资料

[决策树（Decision Tree）](https://zhuanlan.zhihu.com/p/361464944?ivk_sa=1024320u)

[分类算法之决策树](https://blog.csdn.net/ex_6450/article/details/126077545)

[ML Notes](https://github.com/zhulei227/ML_Notes)
