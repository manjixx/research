# 集成学习

## 一、简介

> **回顾**

前面已经介绍过了一些模型，它们各有各的优缺点：

- 比如SVM中，虽然它的最大化间隔能带来不错的泛化能力，但如果某些支持向量恰好是异常点，那么它的决策边界可能会错的很离谱；

- 对于决策树，虽然它的非线性拟合能力很强，但如果放纵树的生长，它甚至会为了个别噪声点创建琐碎的规则；

- 而对于logtic回归模型，它往往不会被个别噪声点影响，但它的模型结构简单，对于复杂的非线性可分数据又难以拟合；

......

那么，如果能将多个模型组合，对其取长补短，那么我们就有可能得到一个更加强大的模型，集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到更好的目的。这便是集成学习的初衷，而它们的组合条件，可以简单归结为：**好而不同**

![](https://img-blog.csdnimg.cn/img_convert/51ce204690c5519dbde6a4905caae60b.png)

> **好而不同的解释**

- 首先，单个模型的效果要尽可能的“好”，这个理解起来很自然（虽然理论证明只要比随机的好一点就行，但大部分情况还是选择好的模型）；

- 其次，就是模型与模型之间尽可能的不同，这很好理解，如果大家都是一样的，那对相同的输入，都给出相同的预测，这就没有意义了，它们没法规避同样的“坑”

> **训练方式:boosting & bagging**

根据个体模型的生成方式，大体可以分为两类：

- 一类是boosting算法，它以串行的方式生成基模型，每个模型的训练需要参考前一阶段的结果，比如对上一轮错分的样本调整权重、或对上一轮预测的残差再做训练等，这方面的代表模型有adaboost以及gbdt(梯度提升树)，简单说来如下：

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_boosting%E7%AE%80%E4%BB%8B.png)

- 另一类是bagging算法，它以并行的方式生成模型，通过投票来做决定，比较有代表的就是随机森林，简单说来如下：

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_bagging%E7%AE%80%E4%BB%8B.png)

> **"不同"的方式**

前面讲了组合的条件是：好而不同，以及它们的生成方式：串行和并行，接下来说说**如何做到与众不同**？这可以从3个方面的来源：数据输入、数据输出、模型自身以及其生成方式做讨论：

| 来源\训练方式 | 串行         | 并行                       |
| ------------- | ------------ | -------------------------- |
| 输入          | 样本权重扰动 | 样本抽样扰动、样本特征扰动 |
| 输出          | 残差、梯度   | 输出表示扰动               |
| 模型          | 超参扰动     | 超参扰动                   |

接下来做一下解释：

- 样本权重扰动：adaboost的方式，它**根据上一轮的训练结果，对预测正确的样本降低权重，对训练错误的样本升高权重**；

- 样本抽样扰动：bagging的处理方法，**对每个模型的训练数据进行有放回的抽样**(又叫做bootstrap抽样)；

- 样本特征扰动：随机森林的方式，它不**仅对每个模型做boostrap抽样**，而且**对当前处理的特征属性还要做抽样，以尽可能与其他模型不同；**

- 梯度：这是梯度提升树的训练方式，每一轮的训练目标是损失函数关于上一轮目标的梯度；

- 残差：这是梯度提升树的一个特例，它的**损失函数为平方损失**的情况，每一轮的训练目标是真实目标与上一轮预测结果之间的差值；

- 输出表示扰动：这可以理解为对训练数据随机加入噪声；

- 超参扰动：对同类型的模型，也可以通过调节其超参数，使得其训练出来的模型不同，比如对决策树，可以限制其不同的深度、最少叶子节点数等...

> **如何组合结果**

对预测数据，每个模型都会给出自己的预测结果，那么该如何组合这些结果呢？简单来说主要有如下的三种方式：

- **公平的方式**：即每个模型的话语权相等，对回归任务直接求平均，对分类任务，做投票，选择投票数最多的类别；

- **加权的方式**：根据模型在训练阶段的不同表现，会赋予其不同的权重，最后对其做加权，求结果；

- **stacking的方式**：这是一种比较高级的组合方式，它训练另外一个模型去做组合，即将基模型的输出作为该模型的输入，原始训练数据的目标作为该模型的输出目标

> **小结一下**

这一节没有代码没有公式，主要对集成学习的相关概念做一个梳理，其核心主要是如下两点：

- 1. 如何生成好而不同的模型：“准确性”和“多样性”这两者其实是冲突的，很多时候需要在这两者之间做一个权衡；

- 2. 如何做组合：组合的方式不同，会直接对输出结果造成影响，一般来说，基模型的性能相差不大时可以采用公平的方式做组合，如果性能差异较大，可以采用加权的方式做组合




集成学习有两个主要的问题需要解决:

- 第一是如何得到若干个个体学习器
- 第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。

## 二、集成学习-Adaboosting

### 2.1 Adaboosting Classifier

#### 2.1.1 简介

adaboost是一种boosting方法，它的要点包括如下两方面：

> **模型生成**

每一个基分类器会基于上一轮分类器在训练集上的表现，对样本做权重调整，使得错分样本的权重增加，正确分类的样本权重降低，所以当前轮的训练更加关注于上一轮误分的样本；

> **模型组合**

adaboost是采用的加权投票的方法

简单来说，adaboost算法涉及两种权重的计算：**样本权重**、**分类器权重**，接下来直接讲算法流程

#### 2.1.2 算法流程

> **输入**

训练集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，其中$x_i∈R^n,y_i∈{+1,−1},i=1,2,...,N$

> **输出**

最终分类器G(x)

> **算法流程**

- 初始化训练数据的权重分布：
  $D1=(w_{11},...,w_{1i},...,w_{1N}),w_{1i}=1/N,i=1,2,...,N$
- 对 $m=1,2,...,M$:
  - 使用具有权重分布$Dm$的训练数据集学习，得到基本分类器：$Gm(x)$
  - 计算$Gm(x)$在训练集上的分类误差率：$e_m=\sum^N_{i=1}P(G_m(x_i)≠y_i)=\sum^N_{i=1}w_{mi}I(G_m(x_i)≠y_i)$
  - 计算$G_m(x)$的权重系数：$α_m=\frac{1}{2} \frac{ln{1-e_m}}{e_m}$
  - 更新训练样本权重：$w_{m+1},i=\frac{w_{mi}}{Z_m}\exp(−α_my_iG_m(x_i)),i=1,2,...,N$，  这里$Z_m$是归一化因子
- 基于基分类器，构建最终的分类器：

$ G(x)=sign(\sum_{m=1}^Mα_mG_m(x))$

简单来说大致流程如下：

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_adaboost%E8%AE%AD%E7%BB%83.png)

#### 2.1.3 代码实现

```python
import os
os.chdir('../')
from ml_models import utils
from ml_models.tree import CARTClassifier
import copy
import numpy as np

"""
AdaBoost分类器的实现，封装到ml_models.ensemble
"""

class AdaBoostClassifier(object):
    def __init__(self, base_estimator=None, n_estimators=10, learning_rate=1.0):
        """
        :param base_estimator: 基分类器，允许异质；异质的情况下使用列表传入比如[estimator1,estimator2,...,estimator10],这时n_estimators会失效；
                                同质的情况，单个estimator会被copy成n_estimators份
        :param n_estimators: 基分类器迭代数量
        :param learning_rate: 学习率，降低后续基分类器的权重，避免过拟合
        """
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        if self.base_estimator is None:
            # 默认使用决策树桩
            self.base_estimator = CARTClassifier(max_depth=2)
        # 同质分类器
        if type(base_estimator) != list:
            estimator = self.base_estimator
            self.base_estimator = [copy.deepcopy(estimator) for _ in range(0, self.n_estimators)]
        # 异质分类器
        else:
            self.n_estimators = len(self.base_estimator)

        # 记录estimator权重
        self.estimator_weights = []

    def fit(self, x, y):
        n_sample = x.shape[0]
        sample_weights = np.asarray([1.0] * n_sample)
        for index in range(0, self.n_estimators):
            self.base_estimator[index].fit(x, y, sample_weight=sample_weights)

            indicates = (self.base_estimator[index].predict(x) == y).astype(int)
            # 计算误分率
            error_rate = np.sum([sample_weights[j] * (1.0 - indicates[j]) for j in range(0, n_sample)]) / n_sample

            # 计算权重系数
            alpha_rate = 1.0 / 2.0 * np.log((1 - error_rate) / (error_rate + 1e-7))
            alpha_rate = min(10.0, alpha_rate)
            self.estimator_weights.append(alpha_rate)

            # 更新样本权重
            for j in range(0, n_sample):
                sample_weights[j] = sample_weights[j] * np.exp(-1.0 * alpha_rate * np.power(-1.0, 1 - indicates[j]))
            sample_weights = sample_weights / np.sum(sample_weights) * n_sample
        # 更新estimator权重
        for i in range(0, self.n_estimators):
            self.estimator_weights[i] *= np.power(self.learning_rate, i)

    def predict_proba(self, x):
        # TODO:并行优化
        result = np.sum(
            [self.base_estimator[j].predict_proba(x) * self.estimator_weights[j] for j in
             range(0, self.n_estimators)],
            axis=0)
        return result / result.sum(axis=1, keepdims=True)

    def predict(self, x):
        return np.argmax(self.predict_proba(x), axis=1)
```

```python
#造伪数据
from sklearn.datasets import make_classification
data, target = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=1, n_redundant=0,
                                   n_repeated=0, n_clusters_per_class=1, class_sep=.5,random_state=21)
# 同质
classifier = AdaBoostClassifier(base_estimator=CARTClassifier(max_depth=2),n_estimators=10)
classifier.fit(data, target)
utils.plot_decision_function(data, target, classifier)

#异质
from ml_models.linear_model import LogisticRegression
from ml_models.svm import SVC
classifier = AdaBoostClassifier(base_estimator=[LogisticRegression(),SVC(kernel='rbf',C=5.0),CARTClassifier()])
classifier.fit(data, target)
utils.plot_decision_function(data, target, classifier)

# 权重衰减
classifier = AdaBoostClassifier(base_estimator=[LogisticRegression(),SVC(kernel='rbf',C=5.0),CARTClassifier()],learning_rate=0.5)
classifier.fit(data, target)
utils.plot_decision_function(data, target, classifier)
```

#### 2.1.4 问题讨论

> **基本要求：弱可学习**

注意有个基本要求，那就是{e_m}<0.5，即分类器至少是弱可学习的，这样才能保证$α_m>0$，此时样本的权重调整（如下公式）才有意义，即正确分类的样本权重降低，错误分类的样本权重升高：

$$ w_{m+1,i}=\left\{
\begin{aligned}
\frac{w_{mi}}{z_m}\exp{-α_m}, G_m(x_i)=y_i\\
\frac{w_{mi}}{z_m}\exp{α_m}, G_m(x_i)\not=y_i\\
\end{aligned}
\right.
$$

对于二分类问题，弱可学习其实是很容易保证的，对于$e_m>0.5$的情况，只需要对其预测取反，即可得到$1−e_m<0.5$的错误率

> **基分类器不支持样本权重怎么办**

对于不能支持样本权重训练的基分类器，可以通过样本重采样来实现

### 2.1.5 训练误差分析

这一部分证明**训练误差会随着基分类器的数量增加而指数下降**，首先抛出第一个不等式关系：

关系式1：$\frac{1}{N}\sum_{i=1}^NI(G(x_i)≠y_i)≤\frac{1}{N}\sum_{i=1}^N\exp(−y_if(x_i))=\prod_{m=1}^MZ_m$

这里$f(x)=\sum^M_{m=1}α_mG_m(x),G(x)=sign(f(x)),Z_m$与上面的定义一样，前半部分很好证明：
如果$G(x_i)≠y_i$，则$y_if(x_i)<0$，所以$\exp(−y_if(x_i))≥1=I(G(x_i)≠y_i)$，而对于$G(x_i)=y_i$的情况，显然有$\exp(−y_if(x_i))≥0=I(G(x_i≠y_i))$；

接下来证明后半部分，根据之前的推导，有如下的两点条件需要注意：

条件1:${w_{1i}=\frac{1}{N},i=1,2,...,N}$
条件2:$w_{mi}\exp(−α_my_iG_m(x_i))=Z_mw_{m+1},i,i=1,2,...,N,m=1,2,...,M$

所以：

$\frac{1}{N}\sum_{i=1}^N\exp(−y_if(x_i))$

$=\frac{1}{N}\sum_{i=1}^N\exp(−\sum_{m=1}^Mα_my_iG_m(x_i)))$

$=\sum_{i=1}^N\frac{1}{N}\prod_{m=1}^M\exp(−α_my_iG_m(x_i))$

$=\sum_{i=1}^Nw_{1i}\prod_{m=1}^M\exp(−α_my_iG_m(x_i))(用到了条件1)$

$=\sum_{i=1}^Nw_{1i}\exp(−α_1y_iG_1(x_i))\prod_{m=2}^M\exp(−α_my_iG_m(x_i))$

$=\sum_{i=1}^NZ_1w_{2i}\prod_{m=2}^M\exp(−α_my_iG_m(x_i))(用到了条件2)$

$=Z_1\sum_{i=1}^Nw_{2i}\prod_{m=2}^M\exp(−α_my_iG_m(x_i))$

$=Z_1Z_2\sum_{i=1}^Nw_{3i}\prod_{m=3}^M\exp(−α_my_iG_m(x_i))$

$=⋯$

$=\prod_{m=1}^MZ_m$
接下来要抛出第二个关系式，对于二分类问题有如下不等式成立：

关系式2：$\prod_{m=1}^MZ_m=\prod_{m=1}^M[2\sqrt{e_m(1−e_m)}]$
$=\prod_{m=1}^M\sqrt{1−4γ^2_m} ≤ \exp(−2\sum_{i=1}^Mγ^2_m)$

这里：$γ_m=\frac{1}{2}−e_m$，首先证明等式部分，由前面的算法部分，我们知道$e_m=\sum^N_{i=1}w_{mi}I(G_m(x_i)≠yi)$，所以：

$Z_m=\sum_{i=1}^Nw_{mi}\exp(−α_my_iG_m(x_i))$
$=\sum_{y_i=G_m(x_i)}w_{mi}e^{−αm^+\sum_{yi≠G_m(x_i)}w_{mi}e^{α_m}$
$=(1−e_m)e^{−α_m}+e_me^{α_m}$
$=2\sqrt{e_m(1−e_m)}$
$=\sqrt{1−4γ^2_m}$

至于不等式部分，其实对于$∀0≤x≤1$，都有$e^{−x/2}≥\sqrt{1-x}$恒成立（证明从略，直观理解如下图），将$x$替换为$4γ^2_m$即可得到上面的不等式，从而关系式2得到证明；

接下来简单做一个推论：一定能找到一个$γ>0$，对所有$γ_m≥γ$成立，则有如下关系：

关系式3：$exp(−2\sum_{i=1}^Mγ^2_m)≤\exp(−2Mγ^2)$
结合关系式1、2、3可以得出：

$\frac{1}{N}\sum_{i=1}^NI(G(x_i)≠y^i)≤\exp(−2Mγ^2)$

即adaboost的误差上界会随着M的增加以指数速率下降

```python
import matplotlib.pyplot as plt
x=np.linspace(0,1,10)
plt.plot(x,np.sqrt(1-x),'b')
plt.plot(x,np.exp(-0.5*x),'r')
```

### 2.2 Adaboosting Regressor

## 三、GBM(Gradient Boosting Machine)

### 3.1 GBM概述

### 3.2 GBDT Regressor

### 3.3 GBDT Classifier

## 四、Bagging Tree

## 五、 RandomForest Tree

### 5.1 简介

为了让学习器越发的不同，randomforest的思路是在bagging的基础上再做一次特征的随机抽样，大致流程如下：

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_randomforest.png)

### 5.2 RandomForest：分类实现

```python
import os
os.chdir('../')
from ml_models import utils
from ml_models.tree import CARTClassifier
import copy
import numpy as np

"""
randomforest分类实现，封装到ml_models.ensemble
"""

class RandomForestClassifier(object):
    def __init__(self, base_estimator=None, n_estimators=10, feature_sample=0.66):
        """
        :param base_estimator: 基学习器，允许异质；异质的情况下使用列表传入比如[estimator1,estimator2,...,estimator10],这时n_estimators会失效；
                                同质的情况，单个estimator会被copy成n_estimators份
        :param n_estimators: 基学习器迭代数量
        :param feature_sample:特征抽样率
        """
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        if self.base_estimator is None:
            # 默认使用决策树
            self.base_estimator = CARTClassifier()
        # 同质分类器
        if type(base_estimator) != list:
            estimator = self.base_estimator
            self.base_estimator = [copy.deepcopy(estimator) for _ in range(0, self.n_estimators)]
        # 异质分类器
        else:
            self.n_estimators = len(self.base_estimator)
        self.feature_sample = feature_sample
        # 记录每个基学习器选择的特征
        self.feature_indices = []

    def fit(self, x, y):
        # TODO:并行优化
        n_sample, n_feature = x.shape
        for estimator in self.base_estimator:
            # 重采样训练集
            indices = np.random.choice(n_sample, n_sample, replace=True)
            x_bootstrap = x[indices]
            y_bootstrap = y[indices]
            # 对特征抽样
            feature_indices = np.random.choice(n_feature, int(n_feature * self.feature_sample), replace=False)
            self.feature_indices.append(feature_indices)
            x_bootstrap = x_bootstrap[:, feature_indices]
            estimator.fit(x_bootstrap, y_bootstrap)

    def predict_proba(self, x):
        # TODO:并行优化
        probas = []
        for index, estimator in enumerate(self.base_estimator):
            probas.append(estimator.predict_proba(x[:, self.feature_indices[index]]))
        return np.mean(probas, axis=0)

    def predict(self, x):
        return np.argmax(self.predict_proba(x), axis=1)
```

```python
#造伪数据
from sklearn.datasets import make_classification
data, target = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=1, n_redundant=0,
                                   n_repeated=0, n_clusters_per_class=1, class_sep=.5,random_state=21)
#同质
classifier = RandomForestClassifier(feature_sample=0.6)
classifier.fit(data, target)
utils.plot_decision_function(data, target, classifier)

#异质
from ml_models.linear_model import LogisticRegression
from ml_models.svm import SVC
classifier = RandomForestClassifier(base_estimator=[LogisticRegression(),SVC(kernel='rbf',C=5.0),CARTClassifier(max_depth=2)],feature_sample=0.6)
classifier.fit(data, target)
utils.plot_decision_function(data, target, classifier)

```

### 5.3 代码实现：回归

```python
from ml_models.tree import CARTRegressor

"""
random forest回归实现，封装到ml_models.ensemble
"""

class RandomForestRegressor(object):
    def __init__(self, base_estimator=None, n_estimators=10, feature_sample=0.66):
        """
        :param base_estimator: 基学习器，允许异质；异质的情况下使用列表传入比如[estimator1,estimator2,...,estimator10],这时n_estimators会失效；
                                同质的情况，单个estimator会被copy成n_estimators份
        :param n_estimators: 基学习器迭代数量
        :param feature_sample:特征抽样率
        """
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        if self.base_estimator is None:
            # 默认使用决策树
            self.base_estimator = CARTRegressor()
        # 同质
        if type(base_estimator) != list:
            estimator = self.base_estimator
            self.base_estimator = [copy.deepcopy(estimator) for _ in range(0, self.n_estimators)]
        # 异质
        else:
            self.n_estimators = len(self.base_estimator)
        self.feature_sample = feature_sample
        # 记录每个基学习器选择的特征
        self.feature_indices = []

    def fit(self, x, y):
        # TODO:并行优化
        n_sample, n_feature = x.shape
        for estimator in self.base_estimator:
            # 重采样训练集
            indices = np.random.choice(n_sample, n_sample, replace=True)
            x_bootstrap = x[indices]
            y_bootstrap = y[indices]
            # 对特征抽样
            feature_indices = np.random.choice(n_feature, int(n_feature * self.feature_sample), replace=False)
            self.feature_indices.append(feature_indices)
            x_bootstrap = x_bootstrap[:, feature_indices]
            estimator.fit(x_bootstrap, y_bootstrap)

    def predict(self, x):
        # TODO:并行优化
        preds = []
        for index, estimator in enumerate(self.base_estimator):
            preds.append(estimator.predict(x[:, self.feature_indices[index]]))

        return np.mean(preds, axis=0)
```

```python
#构造数据
data = np.linspace(1, 10, num=100)
target1 = 3*data[:50] + np.random.random(size=50)*3#添加噪声
target2 = 3*data[50:] + np.random.random(size=50)*10#添加噪声
target=np.concatenate([target1,target2])
data = data.reshape((-1, 1))
#同质
import matplotlib.pyplot as plt
model=RandomForestRegressor(base_estimator=CARTRegressor(),n_estimators=2,feature_sample=1)#feature就一列，没办法...
model.fit(data,target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

```python
#异质
from ml_models.linear_model import LinearRegression
model=RandomForestRegressor(base_estimator=[LinearRegression(),CARTRegressor()],feature_sample=1)
model.fit(data,target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

## 六、Bagging 高阶 Stacking

## 七、Xgboost

### 7.1 Xgbost 简介

#### 7.1.1 简介

![](https://www.researchgate.net/publication/345327934/figure/fig3/AS:1022810793209856@1620868504478/Flow-chart-of-XGBoost.png)

xgboost在集成学习中占有重要的一席之位，通常在各大竞赛中作为杀器使用，同时它在工业落地上也很方便，目前针对大数据领域也有各种分布式实现版本，比如`xgboost4j-spark,xgboost4j-flink`等。`xgboost`的基础也是`gbm`，即梯度提升模型，它在此基础上做了进一步优化...

#### 7.1.2 损失函数：引入二阶项

`xgboost`的损失函数构成如下，即一个**经验损失项+正则损失项**：

$Cost(y,F_{m−1},f_m)=\sum_{i=1}^nL(y_i,F_{m−1}(x_i)+f_m(x_i))+Ω(f_m)$

这里`n`表示样本数，$F_{m−1}$表示前$m−1$轮模型，$f_m$表示第$m$轮新训练模型，所以$F_m=F_{m−1}+f_m，Ω(f_m)$是对第m轮新训练模型进行约束的正则化项，在前面第6小节做过探索，对损失函数近似做二阶泰勒展开，并对近似损失函数做优化，通常会收敛的更快更好

接下里看下对第i个样本的经验项损失函数做二阶展开：

$L(y_i,F_{m−1}(x_i)+f_m(x_i))=L(y_i,F_{m−1}(x_i))+g_if_m(x_i)+\frac{1}{2}h_if^2_m(x_i)$

这里：

$g_i=\frac{∂L(yi,F_{m−1}(x_i))}{∂F_{m−1}(x_i)}$

$h_i=\frac{∂2L(yi,F_{m−1}(x_i))}{∂F_{m−1}(x_i)^2}$

对于第m轮，$L(y_i,F_{m−1}(x_i))$为常数项，不影响优化，可以省略掉，所以损失函数可以表示为如下：

$Cost(y,F_{m−1},f_m)=\sum_{i=1}^n[g_if_m(x_i)+\frac{1}{2}h_if^2_m(x_i)]+Ω(f_m)$

这便是`xgboost`的学习框架，针对不同问题，比如回归、分类、排序，会有不同的`L(⋅)`以及`Ω(⋅)`，另外由于需要二阶信息，所以`L(⋅)`必须要能二阶可微，接下来对基学习器为决策树的情况做推导

#### 7.1.3 基学习器：回归决策树

下面推导一下基学习器为回归树的情况，当选择决策树时，它的正则化项如下：

$Ω(f_m)=γT+\frac{1}{2}λ\sum_{j=1}Tω^2_j$

其中，$j=1,2,...,T$表达对应的叶节点编号，ω_j表示落在第j个叶节点的样本的预测值，即：

$ω_j=f_m(x_i),x_i∈I_j$

$I_j$表示第$j$个叶子节点所属区域，所以决策树的损失函数可以改写为如下：

$Cost(y,F_{m−1},f_m)=\sum_{j=1}^T[(\sum_{i∈I_j}g_i)ω_j+\frac{1}{2}(\sum_{i∈I_j}h_i+λ)ω^2_j]+γT$

这其实是关于`ω`的一元二次函数，直接写出它的最优解：

$ω^∗_j=\frac{−G_j}{H_j+λ}$

这里$G_j=\sum_{i∈I_j}g_i,H_j=\sum_{i∈I_j}h_i$,可见$L2$正则项起到了缩小叶子节点权重的效果，减少其对整个预测结果的影响，从而防止过拟合，将$ω^∗_j$带入可得损失值：

$Cost(y,F_{m−1},f^∗_m)=−\frac{1}{2}\sum_{j=1}^T\frac{G^2_j}{H_j+λ}+γT$

> **特征选择**

很显然，上面的损失函数可以直接用于特征选择中，对某节点在分裂前的评分为：

$Score_{pre}=−\frac{1}{2}\frac{G^2}{H + λ} + γ$
分裂后，左右子节点的评分和为：

$Score_{pre}=−\frac{1}{2}(\frac{G^2}{H_L + λ} + \frac{G^2}{H_R + λ}） +2γ$

所以分裂所能带来的增益：

$Score=\frac{1}{2}[\frac{G^2_L}{H_L + λ} + \frac{G_2^R}{H_R+λ} - \frac{G^2}{H + λ}]−γ$

这里$G=G_L+G_R,H=H_L+H_R$

#### 7.1.4 xgboost中回归树的代码实现

这部分对`xgboost中的回归树`做简单实现，大体流程其实与**CART回归树差不多**，下面说下它与CART回归树不一样的几个点：

- 这里fit与之前的CART回归树有些不一样了，之前是`fit(x,y)`，而现在需要`fit(x,g,h)`；
- 特征选择不一样了，之前是求平方误差的增益，现在需要**利用一阶和二阶导数信息**，见上面的`Score`
- 叶子节点的预测值不一样了，之前是求均值，现在需利用一阶和二阶导数信息，见上面的$w^∗_j$

接下来对xgboost所需要用到的回归树做简单实现

```python
import os
os.chdir('../')
import numpy as np
from ml_models.wrapper_models import DataBinWrapper

"""
xgboost基模型：回归树的实现，封装到ml_models.ensemble
"""

class XGBoostBaseTree(object):
    class Node(object):
        """
        树节点，用于存储节点信息以及关联子节点
        """

        def __init__(self, feature_index: int = None, feature_value=None, y_hat=None, score=None,
                     left_child_node=None, right_child_node=None, num_sample: int = None):
            """
            :param feature_index: 特征id
            :param feature_value: 特征取值
            :param y_hat: 预测值
            :param score: 损失函数值
            :param left_child_node: 左孩子结点
            :param right_child_node: 右孩子结点
            :param num_sample:样本量
            """
            self.feature_index = feature_index
            self.feature_value = feature_value
            self.y_hat = y_hat
            self.score = score
            self.left_child_node = left_child_node
            self.right_child_node = right_child_node
            self.num_sample = num_sample

    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, gamma=1e-2, lamb=1e-1,
                 max_bins=10):
        """
        :param max_depth:树的最大深度
        :param min_samples_split:当对一个内部结点划分时，要求该结点上的最小样本数，默认为2
        :param min_samples_leaf:设置叶子结点上的最小样本数，默认为1
        :param gamma:即损失函数中的gamma
        :param lamb:即损失函数中lambda
        """
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.gamma = gamma
        self.lamb = lamb

        self.root_node: self.Node = None
        self.dbw = DataBinWrapper(max_bins=max_bins)

    def _score(self, g, h):
        """
        计算损失损失评分
        :param g:一阶导数
        :param h: 二阶导数
        :return:
        """
        G = np.sum(g)
        H = np.sum(h)
        return -0.5 * G ** 2 / (H + self.lamb) + self.gamma

    def _build_tree(self, current_depth, current_node: Node, x, g, h):
        """
        递归进行特征选择，构建树
        :param x:
        :param y:
        :param sample_weight:
        :return:
        """
        rows, cols = x.shape
        # 计算G和H
        G = np.sum(g)
        H = np.sum(h)
        # 计算当前的预测值
        current_node.y_hat = -1 * G / (H + self.lamb)
        current_node.num_sample = rows
        # 判断停止切分的条件
        current_node.score = self._score(g, h)

        if rows < self.min_samples_split:
            return

        if self.max_depth is not None and current_depth > self.max_depth:
            return

        # 寻找最佳的特征以及取值
        best_index = None
        best_index_value = None
        best_criterion_value = 0
        for index in range(0, cols):
            for index_value in sorted(set(x[:, index])):
                left_indices = np.where(x[:, index] <= index_value)
                right_indices = np.where(x[:, index] > index_value)
                criterion_value = current_node.score - self._score(g[left_indices], h[left_indices]) - self._score(
                    g[right_indices], h[right_indices])
                if criterion_value > best_criterion_value:
                    best_criterion_value = criterion_value
                    best_index = index
                    best_index_value = index_value

        # 如果减少不够则停止
        if best_index is None:
            return
        # 切分
        current_node.feature_index = best_index
        current_node.feature_value = best_index_value
        selected_x = x[:, best_index]

        # 创建左孩子结点
        left_selected_index = np.where(selected_x <= best_index_value)
        # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
        if len(left_selected_index[0]) >= self.min_samples_leaf:
            left_child_node = self.Node()
            current_node.left_child_node = left_child_node
            self._build_tree(current_depth + 1, left_child_node, x[left_selected_index], g[left_selected_index],
                             h[left_selected_index])
        # 创建右孩子结点
        right_selected_index = np.where(selected_x > best_index_value)
        # 如果切分后的点太少，以至于都不能做叶子节点，则停止分割
        if len(right_selected_index[0]) >= self.min_samples_leaf:
            right_child_node = self.Node()
            current_node.right_child_node = right_child_node
            self._build_tree(current_depth + 1, right_child_node, x[right_selected_index], g[right_selected_index],
                             h[right_selected_index])

    def fit(self, x, g, h):
        # 构建空的根节点
        self.root_node = self.Node()

        # 对x分箱
        self.dbw.fit(x)

        # 递归构建树
        self._build_tree(1, self.root_node, self.dbw.transform(x), g, h)

    # 检索叶子节点的结果
    def _search_node(self, current_node: Node, x):
        if current_node.left_child_node is not None and x[current_node.feature_index] <= current_node.feature_value:
            return self._search_node(current_node.left_child_node, x)
        elif current_node.right_child_node is not None and x[current_node.feature_index] > current_node.feature_value:
            return self._search_node(current_node.right_child_node, x)
        else:
            return current_node.y_hat

    def predict(self, x):
        # 计算结果
        x = self.dbw.transform(x)
        rows = x.shape[0]
        results = []
        for row in range(0, rows):
            results.append(self._search_node(self.root_node, x[row]))
        return np.asarray(results)
```

下面简单测试一下功能，假设`F0(x)=0`，损失函数为平方误差的情况，则其一阶导为$g=F_0(x)−y=−y$，二阶导为$h=1$

```python
#构造数据
data = np.linspace(1, 10, num=100)
target1 = 3*data[:50] + np.random.random(size=50)*3#添加噪声
target2 = 3*data[50:] + np.random.random(size=50)*10#添加噪声
target=np.concatenate([target1,target2])
data = data.reshape((-1, 1))
import matplotlib.pyplot as plt
%matplotlib inline
model=XGBoostBaseTree(lamb=0.1,gamma=0.1)
model.fit(data,-1*target,np.ones_like(target))
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

分别看看lambda和gamma的效果

```python
model=XGBoostBaseTree(lamb=1,gamma=0.1)
model.fit(data,-1*target,np.ones_like(target))
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

```python
model=XGBoostBaseTree(lamb=0.1,gamma=100)
model.fit(data,-1*target,np.ones_like(target))
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

### 7.2 Xgboost 回归

#### 7.1.1 损失函数- squarederror 与 logistic

这一节对xgboost回归做介绍，xgboost共实现了5种类型的回归，分别是squared error、logistic、poisson、gamma、tweedie回归，下面主要对前两种进行推导实现，剩余三种放到下一节

> **squared error**

即损失函数为平方误差的回归模型：

$L(y,ŷ )=\frac{1}{2}(y−ŷ )^2$
所以一阶导和二阶导分别为：

$\frac{∂L(y,ŷ )}{∂ŷ} =ŷ −y$

$\frac{∂^2L(y,ŷ )}{∂ŷ^2}=1.0$

> **logistic**

由于是回归任务，所以y也要套上$sigmoid$函数（用$σ(⋅)$表示），损失函数：

$L(y,ŷ )=(1−σ(y))log(1−σ(ŷ ))+σ(y)log(σ(ŷ ))$

一阶导和二阶导分别为：

$\frac{∂L(y,ŷ)}{∂ŷ} =σ(ŷ)−σ(y)$

$\frac{∂^2L(y,ŷ)}{∂ŷ^2}=σ(ŷ)(1−σ(ŷ))$

#### 7.1.2 损失函数-poisson、gamma、tweedie

这一节将树模型的预测与概率分布相结合，我们假设树模型的输出服从某一分布，而我们的目标是使得该输出的概率尽可能的高，如下图所示

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0_%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.png)

而概率值最高的点通常由分布中的某一个参数（通常是均值）反映，所以我们将树模型的输出打造为分布中的该参数项，然后让树模型的输出去逼近极大似然估计的结果即可，即：

$ŷ →μ_{ML}$

下面分别介绍possion回归，gamma回归，tweedie回归，负二项回归的具体求解

> **泊松回归**

泊松分布的表达式如下：

$P(y∣λ)=\frac{λ^y}{y!}e^{−λ}$

其中，$y$是我们的目标输出，$λ$为模型参数，且$λ$恰为该分布的均值，由于泊松分布要求$y>0$，所以我们对$ŷ$取指数去拟合$λ$，即令：

$λ=e^ŷ$

对于N个样本，其似然函数可以表示如下：

$\prod_{i=1}^N\frac{e^{y_iŷ_i}e^{−e^{ŷ_i}}}{y_i!}$

由于$y_i!$是常数，可以去掉，并对上式取负对数，转换为求极小值的问题：

$L(y,ŷ )=\sum_{i=1}^N(e^{ŷ_i}−y_i\hat{y}_i)$

所以，一阶导和二阶导分别为：

$\frac{∂L(y,ŷ)}{∂ŷ} =e^ŷ −y$

$\frac{∂^2L(y,ŷ)}{∂ŷ^2}=e^ŷ $

> **gamma回归**

gamma分布如下：

$p(y∣α,λ)=\frac{1}{Γ(α)λ^α}y^{α−1}e^{−y/λ}$

其中，$y>0$为我们的目标输出，$α$为形状参数，$λ$为尺度参数，$Γ(⋅)$为$Gamma$函数（后续推导这里会被省略，所以就不列出来了），而$Gamma$分布的均值为$αλ$，这里不好直接变换，我们令$α=1/ϕ,λ=ϕμ$，所以现在$Gamma$分布的均值可以表示为$μ$，此时的$Gamma$分布为：

$p(y∣μ,ϕ)=\frac{1}{yΓ(1/ϕ)}(\frac{y}{μϕ})^{1/ϕ}\exp[−\frac{y}{μϕ}]$

此时，$μ$看做$Gamma$分布的均值参数，而$ϕ$为它的离散参数，在均值给定的情况下，若离散参数越大，$Gamma$分布的离散程度越大，接下来对上面的表达式进一步变换：

$p(y∣μ,ϕ)=\exp[\frac{−y/μ−lnμ}{ϕ}+\frac{1−ϕ}{ϕ}lny−\frac{lnϕ}{ϕ}−lnΓ(\frac{1}{ϕ})]$

同泊松分布一样，我们可以令：

$μ=e^ŷ$

又由于μ与ϕ无关，所以做极大似然估计时可以将ϕ看做常数，我们将对数似然函数的负数看做损失函数，可以写作如下：

$L(y,ŷ)=\sum_{i=1}^N(\frac{y_i}{e^{ŷ_i}} + ŷ_i)$

所以，一阶导和二阶导就可以写出来啦：

$\frac{∂L(y,ŷ)}{∂ŷ}=1−ye^{−ŷ}$

$\frac{∂^2L(y,ŷ)}{∂ŷ^2}=ye^{−ŷ}$

注意：上面的两个向量是按元素相乘

> **tweedie回归**

`tweedie`回归是多个分布的组合体，包括gamma分布，泊松分布，高斯分布等，`tweedie`回归由一个超参数p控制，p不同，则其对应的对数似然函数也不同：

$$ g(y,ϕ)=\left\{
\begin{aligned}
\frac{1}{ϕ}(ylog(μ)−\frac{μ^2−p}{2- p}), p = 1\\
\frac{1}{ϕ}(y\frac{μ^{1- p}}{1 -p} - log(μ)), p = 2\\
\frac{1}{ϕ}(y\frac{μ^{1- p}}{1 -p}−\frac{μ^2−p}{2- p}), p \not= 1, p \not= 2\\
\end{aligned}
\right.
$$

同样的，我们可以令：

$μ=e^ŷ$

由于除开μ以外的都可以视作常数项，所以损失函数可以简化为：

$$ L(y,ŷ )=\left\{
\begin{aligned}
\sum_{i=1}^n(\frac{e^{\hat{y}_i}{(2−p)}}{2 - p}−y_i\hat{y}_i)=\sum^n_{i=1}(e^{\hat{y}_i}−y_i\hat{y}_i), p = 1\\
\sum^n_{i=1}(\hat{y}_i+y_ie^{−\hat{y}_i}), p = 2\\
\sum^n_{i=1}(\frac{exp[\hat{y}_i(2−p)]}{2−p}−y_i\frac{exp[\hat{y}_i(1−p)]}{1−p})), p \not= 1, p \not= 2\\
\end{aligned}
\right.
$$

所以，一阶导：
$$\frac{∂L(y,ŷ)}{∂ŷ}=\left\{
\begin{aligned}
e^ŷ−y,p=1\\
1-ye^{-\hat{y}}, p= 2\\
e^{ŷ(2−p)}−ye^{ŷ (1−p)},p≠1,p≠2\\
\end{aligned}
\right.
$$

二阶导：
$$ \frac{∂^2L(y,ŷ)}{∂ŷ}=\left\{
\begin{aligned}
e^ŷ,p = 1\\
ye^{−ŷ},p=2\\
(2−p)e^{ŷ(2−p)}−(1−p)ye^{ŷ(1−p)}p≠1,p≠2\\
\end{aligned}
\right.
$$

#### 7.1.3 代码实现

具体流程与**gbdt的回归类似**，只是每次要计算一阶、二阶导数信息，同时基学习器要替换为上一节的xgboost回归树

```python
import os
os.chdir('../')
import matplotlib.pyplot as plt
%matplotlib inline
from ml_models.ensemble import XGBoostBaseTree
from ml_models import utils
import copy
import numpy as np

"""
xgboost回归树的实现，封装到ml_models.ensemble
"""

class XGBoostRegressor(object):
    def __init__(self, base_estimator=None, n_estimators=10, learning_rate=1.0, loss='squarederror', p=2.5):
        """
        :param base_estimator: 基学习器
        :param n_estimators: 基学习器迭代数量
        :param learning_rate: 学习率，降低后续基学习器的权重，避免过拟合
        :param loss:损失函数，支持squarederror、logistic、poisson,gamma,tweedie
        :param p:对tweedie回归生效
        """
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        if self.base_estimator is None:
            # 默认使用决策树桩
            self.base_estimator = XGBoostBaseTree()
        # 同质分类器
        if type(base_estimator) != list:
            estimator = self.base_estimator
            self.base_estimator = [copy.deepcopy(estimator) for _ in range(0, self.n_estimators)]
        # 异质分类器
        else:
            self.n_estimators = len(self.base_estimator)
        self.loss = loss
        self.p = p

    def _get_gradient_hess(self, y, y_pred):
        """
        获取一阶、二阶导数信息
        :param y:真实值
        :param y_pred:预测值
        :return:
        """
        if self.loss == 'squarederror':
            return y_pred - y, np.ones_like(y)
        elif self.loss == 'logistic':
            return utils.sigmoid(y_pred) - utils.sigmoid(y), utils.sigmoid(y_pred) * (1 - utils.sigmoid(y_pred))
        elif self.loss == 'poisson':
            return np.exp(y_pred) - y, np.exp(y_pred)
        elif self.loss == 'gamma':
            return 1.0 - y * np.exp(-1.0 * y_pred), y * np.exp(-1.0 * y_pred)
        elif self.loss == 'tweedie':
            if self.p == 1:
                return np.exp(y_pred) - y, np.exp(y_pred)
            elif self.p == 2:
                return 1.0 - y * np.exp(-1.0 * y_pred), y * np.exp(-1.0 * y_pred)
            else:
                return np.exp(y_pred * (2.0 - self.p)) - y * np.exp(y_pred * (1.0 - self.p)), (2.0 - self.p) * np.exp(
                    y_pred * (2.0 - self.p)) - (1.0 - self.p) * y * np.exp(y_pred * (1.0 - self.p))

    def fit(self, x, y):
        y_pred = np.zeros_like(y)
        g, h = self._get_gradient_hess(y, y_pred)
        for index in range(0, self.n_estimators):
            self.base_estimator[index].fit(x, g, h)
            y_pred += self.base_estimator[index].predict(x) * self.learning_rate
            g, h = self._get_gradient_hess(y, y_pred)

    def predict(self, x):
        rst_np = np.sum(
            [self.base_estimator[0].predict(x)] +
            [self.learning_rate * self.base_estimator[i].predict(x) for i in
             range(1, self.n_estimators - 1)] +
            [self.base_estimator[self.n_estimators - 1].predict(x)]
            , axis=0)
        if self.loss in ["poisson", "gamma", "tweedie"]:
            return np.exp(rst_np)
        else:
            return rst_np
```

对 squared error 和 logistics做测试

```python
#测试
data = np.linspace(1, 10, num=100)
target = np.sin(data) + np.random.random(size=100)  # 添加噪声
data = data.reshape((-1, 1))
model = XGBoostRegressor(loss='squarederror')
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
plt.show()

model = XGBoostRegressor(loss='logistic')
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
plt.show()
```

对泊松、gamma、tweedie回归做测试

```python
data = np.linspace(1, 10, num=100)
target = np.sin(data) + np.random.random(size=100) + 1  # 添加噪声
data = data.reshape((-1, 1))
model = XGBoostRegressor(loss='poisson')
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

```python
model = XGBoostRegressor(loss='gamma')
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

```python
model = XGBoostRegressor(loss='tweedie',p=2.5)
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

```python
model = XGBoostRegressor(loss='tweedie',p=1.5)
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

上面的拟合结果，看不出明显区别....,接下来对tweedie分布中p取极端值做一个简单探索...，可以发现取值过大或者过小都有可能陷入欠拟合

```python

model = XGBoostRegressor(loss='tweedie',p=0.1)
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')

model = XGBoostRegressor(loss='tweedie',p=20)
model.fit(data, target)
plt.scatter(data, target)
plt.plot(data, model.predict(data), color='r')
```

### 7.3 Xgboost 分类

#### 7.2.1 简介

xgboost分类分两种情况，二分类和多分类：

- 二分类的思路与logistic回归一样，先对线性函数套一个sigmoid函数，然后再求交叉熵作为损失函数，所以只需要一组回归树并可实现；

- 而多分类的实现，思路同gbm_classifier一样，即同时训练多组回归树，每一组代表一个class，然后对其进行softmax操作，然后再求交叉熵做为损失函数

> 对多分类的情况再推一次损失函数、一阶导、二阶导：

- softmax转换：

$softmax(\hat{y})=softmax([\hat{y_1},\hat{y_2},...,\hat{y_n}])=\frac{1}{\sum^n_{i=1}e^{\hat{y_i}}}[e^{\hat{y_1}},e^{\hat{y_2}},...,e^{\hat{y_n}}]$

- 交叉熵：

$cross\_entropy(y,p)=−\sum_{i=1}^ny_ilogp_i$

将$pi$替换为$\frac{e^{\hat{y_i}}}{\sum^n_{i=1}e^{\hat{y}_i}}$，得到损失函数如下：

$L(yhat,y)=−\sum_{i=1}^ny_ilog\frac{e^{\hat{y}_i}}{\sum^n_{j=1}e^{\hat{x}_j}}$
$=\sum_{i=1}^ny_i(\hat{y}_i−log\sum_{j=1}^ne^{\hat{y}_j})$
$=log\sum_{i=1}^ne^{\hat{y}_i}−\sum_{i=1}^ny_i\hat{y}_i（由于是onehot展开，所以\sum_{i=1}^ny_i=1）$

- 一阶导：

$\frac{∂L(\hat{y},y)}{∂\hat{y}}=softmax([\hat{y_1},\hat{y_2},...,\hat{y_n}])−[y_1,y_2,...,y_n]=softmax(\hat{y})−y$

- 二阶导：

$\frac{∂^2L(\hat{y},y)}{∂\hat{y}^2}=softmax(\hat{y})(1−softmax(\hat{y}))$

#### 7.2.2 代码实现

```python
import os
os.chdir('../')
from ml_models.ensemble import XGBoostBaseTree
from ml_models import utils
import copy
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

"""
xgboost分类树的实现，封装到ml_models.ensemble
"""


class XGBoostClassifier(object):
    def __init__(self, base_estimator=None, n_estimators=10, learning_rate=1.0):
        """
        :param base_estimator: 基学习器
        :param n_estimators: 基学习器迭代数量
        :param learning_rate: 学习率，降低后续基学习器的权重，避免过拟合
        """
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        if self.base_estimator is None:
            self.base_estimator = XGBoostBaseTree()
        # 同质分类器
        if type(base_estimator) != list:
            estimator = self.base_estimator
            self.base_estimator = [copy.deepcopy(estimator) for _ in range(0, self.n_estimators)]
        # 异质分类器
        else:
            self.n_estimators = len(self.base_estimator)

        # 扩展class_num组分类器
        self.expand_base_estimators = []

    def fit(self, x, y):
        # 将y转one-hot编码
        class_num = np.amax(y) + 1
        y_cate = np.zeros(shape=(len(y), class_num))
        y_cate[np.arange(len(y)), y] = 1

        # 扩展分类器
        self.expand_base_estimators = [copy.deepcopy(self.base_estimator) for _ in range(class_num)]

        # 第一个模型假设预测为0
        y_pred_score_ = np.zeros(shape=(x.shape[0], class_num))
        # 计算一阶、二阶导数
        g = utils.softmax(y_pred_score_) - y_cate
        h = utils.softmax(y_pred_score_) * (1 - utils.softmax(y_pred_score_))
        # 训练后续模型
        for index in range(0, self.n_estimators):
            y_pred_score = []
            for class_index in range(0, class_num):
                self.expand_base_estimators[class_index][index].fit(x, g[:, class_index], h[:, class_index])
                y_pred_score.append(self.expand_base_estimators[class_index][index].predict(x))
            y_pred_score_ += np.c_[y_pred_score].T * self.learning_rate
            g = utils.softmax(y_pred_score_) - y_cate
            h = utils.softmax(y_pred_score_) * (1 - utils.softmax(y_pred_score_))

    def predict_proba(self, x):
        # TODO:并行优化
        y_pred_score = []
        for class_index in range(0, len(self.expand_base_estimators)):
            estimator_of_index = self.expand_base_estimators[class_index]
            y_pred_score.append(
                np.sum(
                    [estimator_of_index[0].predict(x)] +
                    [self.learning_rate * estimator_of_index[i].predict(x) for i in
                     range(1, self.n_estimators - 1)] +
                    [estimator_of_index[self.n_estimators - 1].predict(x)]
                    , axis=0)
            )
        return utils.softmax(np.c_[y_pred_score].T)

    def predict(self, x):
        return np.argmax(self.predict_proba(x), axis=1)
````

```python
#造伪数据
from sklearn.datasets import make_classification
data, target = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=1, n_redundant=0,
                                   n_repeated=0, n_clusters_per_class=1, class_sep=.5,random_state=21)
classifier = XGBoostClassifier()
classifier.fit(data, target)
utils.plot_decision_function(data, target, classifier)
```

### 7.4 xgboost优化

> **样本采样与特征采样**

类似于randomforest，xgboost也可进行bootstrap的样本采样，和随机列采样，以增强模型的泛化能力，避免过拟合

> **稀疏/缺失值处理**

xgboost会为稀疏/缺失值选择一个默认方向，如果训练集中有稀疏/缺失值，通过计算其增益来选择往左还是往右作为默认方向，如果训练集中没有，则选择往右为默认方向

> **直方图算法：快速切分点查找**

在构建树时，最重要的操作便是特征及其对应的切分阈值的查找，CART一般选择一种精确的贪心搜索，即遍历所有的特征及其所有可能的取值情况，这非常耗时，xgboost采用了直方图优化策略，具体操作其实很简单，就是对数据做了一个分箱的操作，如下图，将8个连续数值分成了两组，分组前有7个切分点需要搜索，而分组后只有一个切分点，而对于更加一般的情况，连续值的取值情况往往要多的多，分组操作将极大的提高搜索速度，而且还能在一定程度上防止过拟合

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95.png)

而且分箱操作还能在存储和计算上带来好处：

- 存储上，之前需要使用float类型，而分箱后只需要使用int类型，内存使用缩小为18；

- 计算上也可以做优化，比如需要计算右孩子节点的直方图分布，只需使用父节点的直方图分布减去左孩子的直方图分布即可，无需再次统计

另外分箱还有两种策略，全局策略和本地策略：

- 全局策略：全局只做一次分箱操作；

- 本地策略：每个节点在分裂时，都会重新做一次分箱操作；

全局策略操作简单，而本地策略可能取得更好的精度；上面的分箱操作其实是分位数查找法，xgboost还有其他的分位数查找法：

- 误差分位数法：对于数据量太大的情况，没法直接将所有特征取值加载进内容，所以没法精确的求解分位点，该算法>>>构造了一种数据，可以以一定误差保存流式数据的分位点；它的优化版本：A fast algorithm for approximate quantiles in high speed data streams 被用于xgboost中

- 加权分位数法：基本思想是，如果样本的预测结果越不稳定，则需要更细的切分粒度，而二阶导可以用来衡量预测结果的不稳定性，二阶导绝对值越大的样本点越需要被细分

> **leaf-wise vs level-wise**

leaf-wise和level-wise是树的两种生成策略，如下图所示，level-wise是逐层生成决策树，而leaf-wise是按需生成决策树，如果切分后某子节点的收益较低，则不会生成该子节点，这极大的提升了训练速度

![](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/10_level_wise_vs_leaf_wise.png)

>系统层面的优化

这部分内容简略介绍，主要有稀疏数据列压缩、缓存感知访问、外存块计算优化、分布式计算、GPU加速

>稀疏数据列压缩

xgboost会对稀疏数据进行列压缩（Compressed Sparse Column,CSC），这可以提高特征值排序的效率，方便节点分裂时的特征选择

>缓存感知访问

由于每个特征其特征值的排序不同，再对一阶、二阶导数的提取时，会有非连续内存访问的问题，将这些数据放入CPU缓存可以提高计算的效率，具体做法是把某些连续有序的mini-batch数据的统计值缓存到CPU缓存，这样其他特征在做特征选择时，有可能命中这部分数据

> 外存块计算优化

整个特征选择的过程其实是按“列”进行的，也就是说对某列进行操作时，其他的列其实并没有被使用（lable除外），所以树的训练过程并不用将所有数据加载到内存，而是可以按需载入，这部分可以分两个线程来做：一个线程负责计算，另一线性负责数据读取

> 分布式计算

目前xgboost在主流的分布式计算平台上均有实现，比如spark,flink等

> GPU加速

xgboost在按level-wise方式生成树时可以通过gpu提速，通过gpu可以同时对一层的数据进行处理，从而提高最优切分点的查找效率；而节点的桶排序也可通过CUDA原句实现的基数排序优化

## 八、Light GBM

## 九、DART(将dropout应用到了GBDT中)

## 十、树模型的可解释性_模型的特征重要性及样本的特征重要性(sabaas,shap,tree shap)~~~~

## 补充

> Bagging，Boosting二者之间的区别

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。　
> 决策树与这些算法框架进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT
